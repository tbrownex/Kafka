{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b61b96b2",
   "metadata": {},
   "source": [
    "##### https://www.tensorflow.org/io/tutorials/kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be5f2c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-27 21:38:34.003101: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-27 21:38:34.237871: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-27 21:38:34.237896: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-27 21:38:35.069221: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-27 21:38:35.069341: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-27 21:38:35.069353: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/tbrownex/miniconda3/envs/tf/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from kafka import KafkaProducer\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "\n",
    "from getConfig import getConfig\n",
    "from splitData import splitData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13e28272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow-io version: 0.28.0\n",
      "tensorflow version: 2.11.0\n"
     ]
    }
   ],
   "source": [
    "print(\"tensorflow-io version: {}\".format(tfio.__version__))\n",
    "print(\"tensorflow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "234ff19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_COLUMNS = 18     # this is without the label so training columns only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deb285b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_callback(exc):\n",
    "    raise Exception('Error while sendig data to kafka: {0}'.format(str(exc)))\n",
    "\n",
    "def write_to_kafka(topic_name, items):\n",
    "    count = 0\n",
    "    producer = KafkaProducer(bootstrap_servers=['127.0.0.1:9092'])\n",
    "    for message, key in items:\n",
    "        producer.send(topic_name, key=key.encode('utf-8'),\n",
    "                      value=message.encode('utf-8')).add_errback(error_callback)\n",
    "        count+=1\n",
    "    producer.flush()\n",
    "    print(\"Wrote {0} messages into topic: {1}\".format(count, topic_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d230e332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_kafka_item(item):\n",
    "    message = tf.io.decode_csv(item.message, [[0.0] for i in range(NUM_COLUMNS)])\n",
    "    key = tf.strings.to_number(item.key)\n",
    "    return (message, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cb49385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_kafka_test_item(raw_message, raw_key):\n",
    "    message = tf.io.decode_csv(raw_message, [[0.0] for i in range(NUM_COLUMNS)])\n",
    "    key = tf.strings.to_number(raw_key)\n",
    "    return (message, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91220808",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = getConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fc55c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = ['class', 'lepton_1_pT', 'lepton_1_eta', 'lepton_1_phi',\n",
    "           'lepton_2_pT', 'lepton_2_eta', 'lepton_2_phi', 'missing_energy_magnitude',\n",
    "           'missing_energy_phi', 'MET_rel', 'axial_MET', 'M_R', 'M_TR_2', 'R',\n",
    "           'MT2', 'S_R', 'M_Delta_R', 'dPhi_r_b', 'cos(theta_r1)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c394d02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "susy_iterator = pd.read_csv(cfg['dataLoc']+cfg['fileName'],\n",
    "                            header=None,\n",
    "                            names=COLUMNS,\n",
    "                            chunksize=1000)\n",
    "\n",
    "df = next(susy_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d421b11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,000 rows and 19 columns\n",
      "\n",
      "signal to noise ratio: 0.81\n"
     ]
    }
   ],
   "source": [
    "print(\"{:,} rows and {} columns\".format(len(df), df.shape[1]))\n",
    "\n",
    "# See if the classes are balanced. class=0 is \"noise\"; class=1 is \"signal\"\n",
    "noise = len(df[df['class']==0])\n",
    "signal = len(df[df['class']==1])\n",
    "print(\"\\nsignal to noise ratio: {:.2f}\".format(signal/noise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6104b16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = splitData(df, cfg)\n",
    "\n",
    "trainX = d['train'].drop([\"class\"], axis=1)\n",
    "trainY = d['train'][\"class\"]\n",
    "\n",
    "testX = d['test'].drop([\"class\"], axis=1)\n",
    "testY = d['test'][\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "484f6f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataframe to a comma-separated string, skipping the column headers\n",
    "x_train = list(filter(None, trainX.to_csv(index=False).split(\"\\n\")[1:]))\n",
    "y_train = list(filter(None, trainY.to_csv(index=False).split(\"\\n\")[1:]))\n",
    "\n",
    "x_test = list(filter(None, testX.to_csv(index=False).split(\"\\n\")[1:]))\n",
    "y_test = list(filter(None, testY.to_csv(index=False).split(\"\\n\")[1:]))\n",
    "\n",
    "assert(len(x_train)==len(y_train)), 'invalid lengths'\n",
    "assert(len(x_test)==len(y_test)), 'invalid lengths'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5050cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 750 messages into topic: susy-train\n",
      "Wrote 250 messages into topic: susy-test\n"
     ]
    }
   ],
   "source": [
    "# The labels are set as the kafka message keys so as to store data in multiple-partitions\n",
    "# Thus, enabling efficient data retrieval using the consumer groups.\n",
    "write_to_kafka(\"susy-train\", zip(x_train, y_train))\n",
    "write_to_kafka(\"susy-test\", zip(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bc1a340",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-27 21:59:22.255312: W tensorflow_io/core/kernels/audio_video_mp3_kernels.cc:271] libmp3lame.so.0 or lame functions are not available\n",
      "2022-11-27 21:59:22.260318: I tensorflow_io/core/kernels/cpu_check.cc:128] Your CPU supports instructions that this TensorFlow IO binary was not compiled to use: AVX2 FMA\n",
      "2022-11-27 21:59:22.506235: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-11-27 21:59:22.507214: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-11-27 21:59:22.507250: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (basic): /proc/driver/nvidia/version does not exist\n",
      "2022-11-27 21:59:22.511332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-27 21:59:23.259975: I tensorflow_io/core/kernels/kafka_kernels.cc:349] Kafka tail: 750\n"
     ]
    }
   ],
   "source": [
    "train_ds = tfio.IODataset.from_kafka('susy-train', partition=0, offset=0)\n",
    "train_ds = train_ds.shuffle(buffer_size=cfg['ShuffleBufferSize'])\n",
    "train_ds = train_ds.map(decode_kafka_item)\n",
    "train_ds = train_ds.batch(cfg['batchSize'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66713fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.BatchDataset"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85564181",
   "metadata": {},
   "source": [
    "##### Build and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d403fe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZER = \"adam\"\n",
    "LOSS = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "METRICS = ['accuracy']\n",
    "EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff6cb5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               2432      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 68,481\n",
      "Trainable params: 68,481\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(NUM_COLUMNS,)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "721cb837",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1208/1208 [==============================] - 41s 33ms/step - loss: 0.5090 - accuracy: 0.7478\n",
      "Epoch 2/3\n",
      "1208/1208 [==============================] - 40s 33ms/step - loss: 0.4530 - accuracy: 0.7902\n",
      "Epoch 3/3\n",
      "1208/1208 [==============================] - 40s 33ms/step - loss: 0.4473 - accuracy: 0.7925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2fc4d8e3d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=METRICS)\n",
    "model.fit(train_ds, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65a96b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = tfio.experimental.streaming.KafkaGroupIODataset(\n",
    "    topics=[\"susy-test\"],\n",
    "    group_id=\"testcg\",\n",
    "    servers=\"127.0.0.1:9092\",\n",
    "    stream_timeout=10000,\n",
    "    configuration=[\n",
    "        \"session.timeout.ms=7000\",\n",
    "        \"max.poll.interval.ms=8000\",\n",
    "        \"auto.offset.reset=earliest\"\n",
    "    ],\n",
    ")\n",
    "test_ds = test_ds.map(decode_kafka_test_item)\n",
    "test_ds = test_ds.batch(cfg['batchSize'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cfee1cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395/395 [==============================] - 13s 31ms/step - loss: 0.4438 - accuracy: 0.7937\n",
      "test loss, test acc: [0.4438146650791168, 0.7937029600143433]\n"
     ]
    }
   ],
   "source": [
    "res = model.evaluate(test_ds)\n",
    "print(\"test loss, test acc:\", res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6538e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
